{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharindupriyaharshana/H-rverstehenPro/blob/main/H%C3%B6rverstehenPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**HörverstehenPro**\n",
        "\n",
        "This code represents a sophisticated tool designed for helping students, particularly those learning German, to understand spoken German better. It's a Gradio-based web application named \"HörverstehenPro,\" incorporating various AI models for tasks like speech recognition, grammar correction, translation, and grammatical analysis.\n",
        "\n",
        "\n",
        "This tool seems particularly useful for students learning German, enabling them to practice listening, understand spoken German better, correct their grammar, and translate to their native language, all while identifying key parts of speech.\n",
        "\n",
        "Here's a breakdown of each part:"
      ],
      "metadata": {
        "id": "1Up40vwYbr4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsiHw56j0HDA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers.git accelerate datasets[audio]\n",
        "!pip install gradio\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install soundfile\n",
        "!pip install spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCYpyG0AJKye"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports: The code begins by importing necessary libraries and models:\n",
        "\n",
        "\n",
        "\n",
        "*   gradio for creating the web interface.\n",
        "*   soundfile, numpy, difflib, and tempfile for audio file handling and processing.\n",
        "*   transformers, torch, and whisper for utilizing pre-trained AI models.\n",
        "*   spacy for natural language processing (NLP) tasks in German.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j7TY-iqQgDz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The models used**\n",
        "\n",
        "- Whisper Model: A large-v3 model from OpenAI's Whisper series is loaded for speech recognition.\n",
        "\n",
        "- Spacy Model: The de_core_news_sm model from Spacy, tailored for German language processing, is loaded for grammatical analysis.\n",
        "\n",
        "- T5 Grammar Correction Model: A model specialized in correcting German grammar.\n",
        "\n",
        "- Translation Model: This is used to translate German to English, specifically the Helsinki-NLP/opus-mt-de-en model.\n",
        "\n",
        "\n",
        "**Function Definitions:**\n",
        "\n",
        "- correct_grammar: Corrects grammar in a given German text using the T5 model.\n",
        "generate_diff: Generates a textual difference between original and corrected text.\n",
        "- transcribe_and_correct: Transcribes audio to text, corrects its grammar, and shows differences.\n",
        "- transcribe_and_translate: Handles audio input, transcribes it to German text, corrects grammar, translates to English, and extracts nouns and verbs.\n",
        "\n",
        "\n",
        "**Audio Processing:**\n",
        "\n",
        "The code handles audio input, either as a file or a NumPy array, and processes it for transcription and language detection.\n",
        "\n",
        "**Language Detection and Transcription:**\n",
        "\n",
        "Whisper's language detection is hinted to focus on German.\n",
        "The audio is transcribed, and the German text is obtained.\n",
        "\n",
        "**Translation and Grammatical Analysis:**\n",
        "\n",
        "The transcribed German text is translated into English.\n",
        "Spacy's NLP model is used to extract nouns and verbs from the original German text.\n",
        "\n"
      ],
      "metadata": {
        "id": "iWafPQcEgX7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Men1fWaM0RgJ",
        "outputId": "bde887b2-042d-4349-dcfa-e2b2853d2c32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://69c48b5e068b9c5b55.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://69c48b5e068b9c5b55.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1561, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1179, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-1-6707d863ea26>\", line 78, in transcribe_and_translate\n",
            "    audio = whisper.load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1796, in _execute_child\n",
            "    self.pid = _posixsubprocess.fork_exec(\n",
            "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from transformers import pipeline, WhisperForConditionalGeneration, WhisperProcessor\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import difflib\n",
        "import whisper\n",
        "import spacy\n",
        "\n",
        "# Load Whisper large-v3 model\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
        "processor = WhisperProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Load the German model\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "\n",
        "# Initialize Whisper pipeline\n",
        "whisper_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "\n",
        "# Load T5 German Grammar Correction model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aiassociates/t5-small-grammar-correction-german\")\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"aiassociates/t5-small-grammar-correction-german\")\n",
        "\n",
        "# Load the translation model (you can choose an appropriate translation model)\n",
        "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
        "translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
        "\n",
        "\n",
        "def correct_grammar(text):\n",
        "    inputs = tokenizer.encode(\"grammar: \" + text, return_tensors=\"pt\", padding=True)\n",
        "    outputs = t5_model.generate(inputs, max_length=512)\n",
        "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected_text\n",
        "\n",
        "\n",
        "def generate_diff(original, corrected):\n",
        "    diff = difflib.ndiff(original.split(), corrected.split())\n",
        "    diff_text = '\\n'.join(diff)\n",
        "    return diff_text\n",
        "\n",
        "def transcribe_and_correct(audio):\n",
        "    transcribed_text = transcribe(audio)\n",
        "    corrected_text = correct_grammar(transcribed_text)\n",
        "    diff_text = generate_diff(transcribed_text, corrected_text)\n",
        "    return transcribed_text, corrected_text, diff_text\n",
        "\n",
        "\n",
        "def transcribe_and_translate(audio):\n",
        "    # Create a temporary file to save the audio if it's a NumPy array\n",
        "    if isinstance(audio, np.ndarray) or (isinstance(audio, tuple) and isinstance(audio[1], np.ndarray)):\n",
        "        # If audio is a tuple, it contains (sample_rate, audio_data)\n",
        "        if isinstance(audio, tuple):\n",
        "            sample_rate, audio_data = audio\n",
        "        else:\n",
        "            sample_rate = 16000  # the model request in this rate\n",
        "            audio_data = audio\n",
        "\n",
        "        # Write audio data to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
        "            sf.write(tmp_file.name, audio_data, sample_rate)\n",
        "            tmp_file_path = tmp_file.name\n",
        "\n",
        "        # Load the audio file with Whisper\n",
        "        audio = whisper.load_audio(tmp_file_path)\n",
        "    else:\n",
        "        # If it's a file path, use it directly\n",
        "        audio = whisper.load_audio(audio)\n",
        "\n",
        "    # Load audio and pad/trim it to fit 30 seconds\n",
        "    audio_data = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # Make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio_data).to(model.device)\n",
        "\n",
        "\n",
        "     # Set the decoding options with German language\n",
        "    options = whisper.DecodingOptions(language=\"de\")\n",
        "\n",
        "    # Transcribe in the original language (German)\n",
        "    original_transcription_result = whisper_pipeline(audio, generate_kwargs={\"language\": \"german\"})\n",
        "    original_transcribed_text = original_transcription_result[\"text\"]\n",
        "\n",
        "    # Correct grammar in the original transcription\n",
        "    corrected_text = correct_grammar(original_transcribed_text)\n",
        "\n",
        "\n",
        "    # Use the translation model to translate from German to English\n",
        "    input_ids = translation_tokenizer.encode(\"translate German to English: \" + corrected_text, return_tensors=\"pt\", padding=True)\n",
        "    translated_ids = translation_model.generate(input_ids, max_length=512)\n",
        "    translated_text = translation_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    # Extract nouns and verbs using POS tagging\n",
        "\n",
        "    doc = nlp(original_transcribed_text)\n",
        "\n",
        "    # Extract nouns and verbs\n",
        "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
        "\n",
        "\n",
        "    return original_transcribed_text, corrected_text, translated_text, nouns, verbs\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe_and_translate,\n",
        "    inputs=gr.Audio(),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Original Transcribed Text (German)\"),\n",
        "        gr.Textbox(label=\"Grammar Corrected Text\"),\n",
        "        gr.Textbox(label=\"Translated to English\"),\n",
        "        gr.Textbox(label=\"Nouns\"),\n",
        "        gr.Textbox(label=\"Verbs\")\n",
        "    ],\n",
        "    title='HörverstehenPro',\n",
        "    description='Your German Listening Assistant by GIVE A NAME',\n",
        "    live=True\n",
        ")\n",
        "\n",
        "# Launch with sharing enabled\n",
        "iface.launch(share=True, debug=True)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}